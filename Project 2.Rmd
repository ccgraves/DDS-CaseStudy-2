---
title: "Project 2 - Predicting Employee Attrition From HR Data"
author: "Allen Crane, Nicholas Cellini, Quincy Roundtree, Heber Nelson, Carter Graves"
date: "July 25, 2018"
output: html_document
---

```{r setup, echo=TRUE, include=TRUE, cache=FALSE}
options(width=131)
knitr::opts_chunk$set(echo = TRUE)
```

##Section 1: Answers to Case Study 2 Tasks

###Introduction to this project: DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 1000 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they have tasked your data science team to conduct an analysis of existing employee data.

###Our team has been given a dataset (CaseStudy2-Data.xlsx) to conduct exploratory data analysis (EDA) to determine factors that lead to attrition. Our goal is to identify (at least) the top three factors that contribute to turnover. Additionally, the business is also interested in learning about any job role specific trends that may exist in the data set.

###(Note: Repository steps 1A, 1B, and 1C are contained in the Readme file.)

###2. Clean the raw data. Set working directory.
###2A. Read the csv into R and take a look at the data set.  Output how many rows and columns the data.frame is. 

```{r}
#install.packages("csvread")
library(csvread)
setwd("C:\\Users\\allen\\Documents\\SMU Data Science\\MSDS6306 - Doing Data Science\\Case Study 2")
workforce.df <- read.csv("CaseStudy2-data.csv", header=TRUE, stringsAsFactors = TRUE, sep = ",")

#Get number of rows in the dataset
nrow(workforce.df)

#Get number of columns in the dataset
ncol(workforce.df)

#View the dataset
head(workforce.df)

#Summary statistics of the dataset
summary(workforce.df)
```

###2B. The column names are either too much or not enough.  Change the column names so that they do not have spaces, underscores, slashes, and the like. All column names should be under 12 characters. Make sure you're updating your codebook with information on the tidied data set as well.
###(Note: 2C. Some columns are, due to Qualtrics, malfunctioning.)

```{r}
colnames(workforce.df) <- c("Age", "Attrition", "BusTravel", "DayRate", "Department", "DistFromHome", "Education", "EducField", "EmpCount", "EmpNumber", "EnvironSat", "Gender", "HourRate", "JobInvolve", "JobLevel", "JobRole", "JobSat", "MaritalStat", "MonthInc", "MonthRate", "NumCompWrkd", "Over18", "OverTime", "PctSalHike", "PerfRating", "RelationSat", "StdHours", "StkOptLvl", "TotWorkYrs", "TrngTmLstYr", "WrkLifBal", "YrsAtComp", "YrsCurRole", "YrsLstPromo", "YrsCurrMgr")
head(workforce.df)
```

###2D. Make sure your columns are the proper data types (i.e., numeric, character, etc.).  If they are incorrect, convert them. 

```{r}
sapply(workforce.df, class)
```

####After inspecting the class of each of our variables, we see a mix of integer/numeric data and character/factor data. 


###3.Preliminary Analysis. 
###3A.Remove all observations where the participant is under age 18. No further analysis of underage individuals is permitted by your client. Remove any other age outliers as you see fit, but be sure to tell what you're doing and why.

```{r}
summary(workforce.df$Over18)
```

####After inspecting the values in the Over18 variable, we find that there are no individuals that are under 18. We will proceed with the analysis.


###3B.Please provide (in table format or similar), descriptive statistics on at least 7 variables (Age, Monthly Income, etc.).  Create a simple histogram for two of them.  Comment on the shape of the distribution in your markdown.

```{r}
#create descriptive statistics

#install.packages("pastecs")
library(pastecs)
attach(workforce.df)
scores <- cbind(Age, Attrition, BusTravel, Department, DistFromHome, Education, EducField, MonthRate)
options(scipen=100)
options(digits=2)
stat.desc(scores, basic=F)

#create historgrams (Age and Monthly Income)

#install.packages("ggplot2")
library(ggplot2)
hist(workforce.df$Age,
     main="Histogram for Age", 
     xlab="Age", 
     col="blue",
     las=1, 
     breaks=10)

hist(workforce.df$MonthInc,
     main="Histogram for Monthly Income", 
     xlab="Monthly Income",
     xlim=c(2000,20000),
     ylim=c(0,400),
     col="green",
     las=1, 
     breaks=25)
```

####While the Age plot shows a relatively normal "bell curve" distribution, the Monthly Income plot appears to be fairly consistent. One explanation of this could be that there is a standardized set of income bands per level, and that there is a similar count of jobs at most levels.


###3C. Give the frequencies (in table format or similar) for Gender, Education, and Occupation.  They can be separate tables, if that's your choice.

```{r}
ggplot(data.frame(workforce.df),aes(x=Gender)) + geom_bar(fill = "#FF6666") 
ggplot(data.frame(workforce.df),aes(x=Education)) + geom_bar(fill = "#D55E00")
ggplot(data.frame(workforce.df),aes(x=JobRole)) + geom_bar(fill = "#CC79A7") + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
```

###3D.Give the counts (again, table) of management positions.

```{r}

with(droplevels(workforce.df[which(workforce.df$JobRole!= "Healthcare Representative" & workforce.df$JobRole!= "Human Resources" & workforce.df$JobRole!= "Laboratory Technician" & workforce.df$JobRole!= "Research Scientist" & workforce.df$JobRole!= "Sales Representative"),]), summary(JobRole))

with(droplevels(workforce.df[which(workforce.df$JobRole!= "Healthcare Representative" & workforce.df$JobRole!= "Human Resources" & workforce.df$JobRole!= "Laboratory Technician" & workforce.df$JobRole!= "Research Scientist" & workforce.df$JobRole!= "Sales Representative"),]), 
plot(JobRole, las=2, cex.xaxis=0.8))
```


###4.Deeper Analysis and Visualization 
###4A. (Note: You should make all of these appealing looking.  Remember to include things like a clean, informative title, axis labels that are in plain English, and readable axis values that do not overlap.)
###4B. Create bar charts in ggplot or similar. The bars should be in descending order, Use any color palette of your choice other than the default. (Note: See Regression Coefficients output from Predictive Model summary below)


###4C. Is there a relationship between Age and Income?  Create a scatterplot and make an assessment of whether there is a relationship.  Color each point based on the Gender of the participant.  You're welcome to use lm() or similar functions to back up your claims.

```{r}
model2 <- lm(MonthInc ~ Age, data=workforce.df)
summary(model2)

ggplot(workforce.df, aes(x=Age, y=MonthInc, color = Gender)) + geom_point (size = 2, shape = 18) + geom_smooth(method = lm, linetype = "dashed", color = "darkred", fill = "steelblue4") + labs(y = "Monthly Income")
```

####There appears to be strong evidence that Age and Monthly Income are correlated. When Gender is overlaid, there also appears to be some evidence that there may be a difference between Male and Female incomes, but this would require more robust hypothesis testing. 

###4D. What about Life Satisfaction?  Create another scatterplot.  Is there a discernible relationship there to what?

####For our dataset, we conducted an employee survey that asks the overall question of "How Satisfied are You With Your Life?" We then plotted this against Attrition to see the effect of this survey question on Attrition. Since Attrition only has two levels, the plot is not actually a scatter plot, but a box plot on both Attrition = "Yes" and Attrition  = "No". It appears that Life Satisfaction is slightly higher for those that leave the company, suggesting that we have more work to do with our employee retention programs!

```{r}
setwd("C:\\Users\\allen\\Documents\\SMU Data Science\\MSDS6306 - Doing Data Science\\Case Study 2")
workforce_ls.df <- read.csv("CaseStudy2-dataLS.csv", header=TRUE, stringsAsFactors = TRUE, sep = ",")

attach(workforce_ls.df)
plot(Attrition, LifeSatisfaction, main="Attrition by Life Satisfaction", xlab="Attrition", ylab="LifeSatisfaction", pch=19)

```

##Section 2: Building the model to predict Attrition

###The following is our approach to build a predictive model for Attrition. This will form the basis of our recommendation in our presentation.

###Add row number to master, and insert as a unique column into data set. We will break out the master file into a factor data set and a categorical data set. The categorical data set will be flattened into a one hot encoded file. Preserving the ID column allows us something to join on, when we merge these files back together when we build the model.

```{r}
library(tidyverse)
workforce.df <- tibble::rowid_to_column(workforce.df, "ID")
```

###Split data set into factors and categories, including row IDs

```{r}
workforce_factors.df <- workforce.df[c(1,2,3,5,7,10,11,14,20,21,22,25,28,30,31,33,34,35,36)]
workforce_categories.df <- workforce.df[c(1,3,4,6,8,9,12,13,15,16,17,18,19,23,24,25,27,29,32)]
```

###One hot encoding for categorical data

```{r}
#install.packages("data.table")
library(data.table)
workforce_categories_onehot.df <- dcast(melt(workforce_categories.df, id.vars='ID'), ID ~ variable + value, fun = length)
```

###Merge data files together

```{r}
workforce_ohe.df <- merge(workforce_factors.df, workforce_categories_onehot.df, by="ID")
```

###Remove the Attrition categorical variables, because they are perfectly correlated values

```{r}
workforce_ohe.df$Attrition <- NULL
workforce_ohe.df$Attrition_No <- NULL
```

###The following modeling code was adapted from: https://datascienceplus.com/perform-logistic-regression-in-r/

###A visual take on the missing values might be helpful: the Amelia package has a special plotting function missmap() that will plot your dataset and highlight missing values. In our case, this adds nothing meaningful.

```{r}
#library(Amelia)
#missmap(workforce_ohe.df, main = "Missing values vs observed")
```

###We split the data into two chunks: training and testing set. The training set will be used to fit our model which we will be testing over the testing set. Note that the training data set is ~2/3 the size of the total data, because a training set at 1/2 the size of the total data did not converge.

```{r}
workforce_ohe_train.df <- workforce_ohe.df[1:1000,]
workforce_ohe_test.df <- workforce_ohe.df[1001:1470,]
```

###Now, let's fit the model. Be sure to specify the parameter family=binomial in the glm() function. Note that we upped the max iterations to 50, in order to override the default number of iterations. That said, this model converges at 15 iterations.

```{r}
model <- glm(Attrition_Yes ~.,family=binomial(link='logit'),data=workforce_ohe_train.df, control=list(maxit=50))
```

###By using function summary() we obtain the results of our model:

```{r}
summary(model)
```

###In the following steps, we format the model outputs to create a list of the top coefficients, by the absolute Z Score values.

```{r}
#Output Coefficients of the model into a data frame. We will format these to graph the top Coefficient values
#install.packages("broom")
library(broom)
model_output.df <- tidy(model)

#Create Model Output 2 file of Coefficients and Z Scores
model_output2.df <- data.frame(model_output.df$term, model_output.df$statistic)
colnames(model_output2.df) <- c("term", "statistic")
head(model_output2.df)

#Create Model Output 3 file of Top 10 Coefficients by absolute value of Z Scores
#install.packages("dplyr")
library(dplyr)
model_output3.df <- top_n(model_output2.df, 10, abs(model_output2.df$statistic))
head(model_output3.df)

#Create Model Output 4 file that sorts the Top 10 Coefficients for graphing
model_output4.df <- model_output3.df[order(model_output3.df$statistic,decreasing = FALSE),]

#Graph the Top 10 Coefficients by Z Score
ylim <- c(-9,5)
barplot(model_output4.df$statistic, col="#3F97D0", main="Top 10 Model Coefficients to Predict Attrition", ylab="Z Score", names.arg=model_output4.df$term, cex.names=0.8, las=2, ylim = ylim)
model_output2.df <- data.frame(model_output.df$term, model_output.df$statistic)
```


##Section 3: Validating the model for fit

###Now we can run the anova() function on the model to analyze the table of deviance

```{r}
anova(model, test="Chisq")
```

###While no exact equivalent to the R2 of linear regression exists, the McFadden R2 index can be used to assess the model fit.

```{r}
#install.packages("pscl")
library(pscl)
pR2(model)
```

###Assessing the predictive ability of the model: In the steps above, we briefly evaluated the fitting of the model, now we would like to see how the model is doing when predicting y on a new set of data. By setting the parameter type='response', R will output probabilities in the form of P(y=1|X). Our decision boundary will be 0.5. If P(y=1|X) > 0.5 then y = 1 otherwise y=0. Note that for some applications different decision boundaries could be a better option.

```{r}
fitted.results <- predict(model,newdata=subset(workforce_ohe_test.df,type='response'))
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != workforce_ohe_test.df$Attrition_Yes)
print(paste('Accuracy',1-misClasificError))
```

###As a last step, we are going to plot the ROC curve and calculate the AUC (area under the curve) which are typical performance measurements for a binary classifier. The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

```{r}
#install.packages("ROCR")
library(ROCR)
p <- predict(model, newdata=subset(workforce_ohe_test.df, type="response"))
pr <- prediction(p, workforce_ohe_test.df$Attrition)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```




